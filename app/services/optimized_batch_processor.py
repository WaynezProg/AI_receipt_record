"""
å„ªåŒ–æ‰¹æ¬¡è™•ç†æœå‹™ - æå‡è™•ç†é€Ÿåº¦çš„æ™ºèƒ½æ‰¹é‡è™•ç†
"""

import asyncio
import time
import uuid
import os
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from loguru import logger
from app.services.ocr_service import ocr_service
from app.services.ai_service import ai_service
from app.services.csv_service import csv_service
from app.services.cache_service import cache_service
from app.services.azure_usage_tracker import azure_usage_tracker
from app.utils.image_utils import image_utils


class OptimizedBatchProcessor:
    """å„ªåŒ–æ‰¹æ¬¡è™•ç†å™¨ - æ™ºèƒ½ä¸¦è¡Œè™•ç†å’Œæœ¬åœ°é è™•ç†"""

    def __init__(self):
        # APIé™åˆ¶è¨­å®š
        self.azure_rate_limit = 20  # Azureæ¯åˆ†é˜20æ¬¡
        self.claude_rate_limit = 50  # Claudeæ¯åˆ†é˜50æ¬¡

        # ä¸¦è¡Œæ§åˆ¶ - ç¬¦åˆAzure F0å…è²»å±¤é™åˆ¶
        self.max_concurrent_azure = 1  # é™ä½åˆ°1å€‹ä¸¦è¡ŒAzureè«‹æ±‚ï¼Œé¿å…429éŒ¯èª¤
        self.max_concurrent_claude = 5  # æœ€å¤§ä¸¦è¡ŒClaudeè«‹æ±‚
        self.batch_size = 10  # å„ªåŒ–çš„æ‰¹æ¬¡å¤§å°

        # å»¶é²æ§åˆ¶ - ç¢ºä¿ç¬¦åˆAzureé™åˆ¶
        self.azure_delay = 4  # å¢åŠ åˆ°4ç§’ï¼Œç¢ºä¿æ¯åˆ†é˜ä¸è¶…é15æ¬¡è«‹æ±‚
        self.claude_delay = 1  # Claudeè«‹æ±‚é–“éš”

        # é€²åº¦è¿½è¹¤
        self.current_progress = 0
        self.total_items = 0
        self.current_batch = 0
        self.total_batches = 0
        self.start_time = None

        # å¿«å–æ§åˆ¶
        self.use_cache = True
        self.skip_enhancement = True  # è·³éåœ–ç‰‡å¢å¼·ä»¥æå‡é€Ÿåº¦

        # æœ¬åœ°é è™•ç†
        self.use_local_preprocessing = True

        # æª”æ¡ˆç®¡ç†
        self.auto_delete_successful = True  # è™•ç†æˆåŠŸå¾Œè‡ªå‹•åˆªé™¤åœ–ç‰‡
        self.keep_failed_files = True  # ä¿ç•™å¤±æ•—çš„æª”æ¡ˆä»¥ä¾¿é‡è©¦

    async def _preprocess_image_local(self, image_path: str) -> str:
        """æœ¬åœ°åœ–ç‰‡é è™•ç† - æ¸›å°‘å°Azureçš„ä¾è³´"""
        try:
            # å¿«é€Ÿåœ–ç‰‡å„ªåŒ–
            optimized_path = await asyncio.get_event_loop().run_in_executor(
                None, self._optimize_image_sync, image_path
            )
            return optimized_path
        except Exception as e:
            logger.warning(f"æœ¬åœ°é è™•ç†å¤±æ•—: {e}")
            return image_path

    def _optimize_image_sync(self, image_path: str) -> str:
        """åŒæ­¥åœ–ç‰‡å„ªåŒ–"""
        try:
            # å¿«é€Ÿèª¿æ•´å¤§å°å’Œæ ¼å¼
            optimized_path = image_utils.resize_image(
                image_path, max_width=1200, max_height=1600
            )
            return optimized_path
        except Exception as e:
            logger.warning(f"åœ–ç‰‡å„ªåŒ–å¤±æ•—: {e}")
            return image_path

    async def _process_ocr_with_retry(self, image_path: str, retries: int = 2) -> Dict:
        """å¸¶é‡è©¦çš„OCRè™•ç†"""
        for attempt in range(retries + 1):
            try:
                # æª¢æŸ¥å¿«å–
                if self.use_cache:
                    # å¾åœ–ç‰‡è·¯å¾‘ä¸­æå–æª”æ¡ˆåç¨±
                    filename = os.path.basename(image_path)
                    cached_result = cache_service.load_ocr_result(filename)
                    if cached_result:
                        logger.info(f"ä½¿ç”¨å¿«å–OCRçµæœ: {filename}")
                        return cached_result.get("ocr_data", {})

                # åŸ·è¡ŒOCR
                result = await ocr_service.extract_text(image_path)

                # ä¿å­˜åˆ°å¿«å–
                if self.use_cache and result.get("success"):
                    filename = os.path.basename(image_path)
                    cache_service.save_ocr_result(filename, result)

                return result

            except Exception as e:
                error_msg = str(e)

                # ç‰¹æ®Šè™•ç†429éŒ¯èª¤ï¼ˆè«‹æ±‚é »ç‡è¶…é™ï¼‰
                if "RATE_LIMIT_EXCEEDED" in error_msg or "429" in error_msg:
                    if attempt < retries:
                        # æŒ‡æ•¸é€€é¿ç­–ç•¥ï¼šç­‰å¾…æ™‚é–“é€æ¼¸å¢åŠ 
                        wait_time = min(10 * (2**attempt), 60)  # æœ€å¤§ç­‰å¾…60ç§’
                        logger.warning(
                            f"Azure APIé »ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦ ({attempt + 1}/{retries})"
                        )
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"OCRè™•ç†å¤±æ•—ï¼ˆé »ç‡é™åˆ¶ï¼‰: {error_msg}")
                        return {
                            "success": False,
                            "error": f"Azure APIé »ç‡é™åˆ¶: {error_msg}",
                        }
                else:
                    if attempt < retries:
                        logger.warning(f"OCRé‡è©¦ {attempt + 1}/{retries}: {error_msg}")
                        await asyncio.sleep(self.azure_delay * (attempt + 1))
                    else:
                        logger.error(f"OCRè™•ç†å¤±æ•—: {error_msg}")
                        return {"success": False, "error": error_msg}

    async def _process_ai_with_retry(self, ocr_result: Dict, filename: str, retries: int = 2) -> Dict:
        """å¸¶é‡è©¦çš„AIè™•ç†ï¼ˆæª¢æŸ¥æš«å­˜ï¼‰"""
        # æª¢æŸ¥æ˜¯å¦æœ‰AIæš«å­˜
        ai_cache_data = cache_service.load_ai_result(filename)
        if ai_cache_data and ai_cache_data.get("receipt_data"):
            logger.info(f"ä½¿ç”¨AIæš«å­˜è³‡æ–™: {filename}")
            # å¾æš«å­˜è³‡æ–™æ¢å¾©ReceiptDataå°è±¡
            from app.models.receipt import ReceiptData
            receipt_dict = ai_cache_data["receipt_data"]
            # è™•ç†æ—¥æœŸå­—ä¸²
            if isinstance(receipt_dict.get("date"), str):
                from datetime import datetime
                try:
                    receipt_dict["date"] = datetime.fromisoformat(receipt_dict["date"])
                except:
                    pass
            return ReceiptData(**receipt_dict)
        
        # æ²’æœ‰æš«å­˜ï¼ŒåŸ·è¡ŒAIè™•ç†
        for attempt in range(retries + 1):
            try:
                # æå–çµæ§‹åŒ–è³‡æ–™
                structured_data = ocr_service.extract_structured_data(ocr_result)

                result = await ai_service.process_receipt_text(
                    ocr_result, structured_data
                )
                # ä¿å­˜åˆ°æš«å­˜
                cache_service.save_ai_result(filename, result, ocr_result)
                return result

            except Exception as e:
                if attempt < retries:
                    logger.warning(f"AIé‡è©¦ {attempt + 1}/{retries}: {e}")
                    await asyncio.sleep(self.claude_delay * (attempt + 1))
                else:
                    logger.error(f"AIè™•ç†å¤±æ•—: {e}")
                    return {"success": False, "error": str(e)}

    async def _process_single_item_optimized(self, filename: str) -> Dict:
        """å„ªåŒ–çš„å–®å€‹é …ç›®è™•ç†"""
        try:
            image_path = f"./data/receipts/{filename}"

            # 1. æœ¬åœ°é è™•ç†
            if self.use_local_preprocessing:
                image_path = await self._preprocess_image_local(image_path)

            # 2. OCRè™•ç†ï¼ˆä¸¦è¡Œæ§åˆ¶ï¼‰
            ocr_result = await self._process_ocr_with_retry(image_path)
            if not ocr_result.get("success"):
                return {"success": False, "error": ocr_result.get("error", "OCRå¤±æ•—")}

            # 3. AIè™•ç†ï¼ˆä¸¦è¡Œæ§åˆ¶ï¼Œæª¢æŸ¥æš«å­˜ï¼‰
            ai_result = await self._process_ai_with_retry(ocr_result, filename)
            if not ai_result or (isinstance(ai_result, dict) and not ai_result.get("success", True)):
                return {"success": False, "error": "AIè™•ç†å¤±æ•—"}

            # 4. è™•ç†æˆåŠŸå¾Œåˆªé™¤åœ–ç‰‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.auto_delete_successful:
                await self._delete_successful_image(filename)

            return {
                "success": True,
                "filename": filename,
                "data": ai_result,
                "ocr_result": ocr_result,
                "processing_time": time.time(),
            }

        except Exception as e:
            logger.error(f"è™•ç†å¤±æ•— {filename}: {e}")
            return {"success": False, "error": str(e)}

    async def _delete_successful_image(self, filename: str):
        """åˆªé™¤è™•ç†æˆåŠŸçš„åœ–ç‰‡"""
        try:
            image_path = f"./data/receipts/{filename}"
            if os.path.exists(image_path):
                os.remove(image_path)
                logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤è™•ç†æˆåŠŸçš„åœ–ç‰‡: {filename}")
            else:
                logger.warning(f"åœ–ç‰‡ä¸å­˜åœ¨ï¼Œç„¡æ³•åˆªé™¤: {filename}")
        except Exception as e:
            logger.error(f"åˆªé™¤åœ–ç‰‡å¤±æ•— {filename}: {e}")

    async def _cleanup_failed_images(self, failed_files: List[Dict]):
        """æ¸…ç†å¤±æ•—çš„åœ–ç‰‡ï¼ˆå¦‚æœè¨­å®šç‚ºä¸ä¿ç•™ï¼‰"""
        if self.keep_failed_files:
            return

        for failed_file in failed_files:
            filename = failed_file.get("filename")
            if filename:
                try:
                    image_path = f"./data/receipts/{filename}"
                    if os.path.exists(image_path):
                        os.remove(image_path)
                        logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤å¤±æ•—çš„åœ–ç‰‡: {filename}")
                except Exception as e:
                    logger.error(f"åˆªé™¤å¤±æ•—åœ–ç‰‡æ™‚å‡ºéŒ¯ {filename}: {e}")

    async def _process_batch_parallel(self, filenames: List[str]) -> List[Dict]:
        """ä¸¦è¡Œè™•ç†æ‰¹æ¬¡"""
        # å‰µå»ºä¿¡è™Ÿé‡ä¾†æ§åˆ¶ä¸¦è¡Œåº¦
        azure_semaphore = asyncio.Semaphore(self.max_concurrent_azure)
        claude_semaphore = asyncio.Semaphore(self.max_concurrent_claude)

        async def process_with_semaphore(filename: str) -> Dict:
            async with azure_semaphore:
                # OCRè™•ç†
                image_path = f"./data/receipts/{filename}"
                if self.use_local_preprocessing:
                    image_path = await self._preprocess_image_local(image_path)

                ocr_result = await self._process_ocr_with_retry(image_path)
                if not ocr_result.get("success"):
                    return {
                        "success": False,
                        "filename": filename,
                        "error": ocr_result.get("error"),
                    }

                # æ·»åŠ å»¶é²ä»¥ç¬¦åˆAPIé™åˆ¶
                await asyncio.sleep(self.azure_delay)

                async with claude_semaphore:
                    # AIè™•ç†
                    ai_result = await self._process_ai_with_retry(ocr_result, filename)
                    await asyncio.sleep(self.claude_delay)

                    if ai_result:
                        return {
                            "success": True,
                            "filename": filename,
                            "data": ai_result,
                        }
                    else:
                        return {
                            "success": False,
                            "filename": filename,
                            "error": "AIè™•ç†å¤±æ•—",
                        }

        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä»»å‹™
        tasks = [process_with_semaphore(filename) for filename in filenames]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # è™•ç†çµæœ
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                processed_results.append({"success": False, "error": str(result)})
            else:
                processed_results.append(result)

        return processed_results

    async def process_large_batch_optimized(
        self, filenames: List[str], save_detailed_csv: bool = True
    ) -> Dict:
        """å„ªåŒ–çš„å¤§æ‰¹é‡è™•ç†"""
        start_time = time.time()
        self.start_time = start_time
        self.total_items = len(filenames)
        self.current_progress = 0

        # åˆ†æ‰¹è™•ç†
        batches = [
            filenames[i : i + self.batch_size]
            for i in range(0, len(filenames), self.batch_size)
        ]
        self.total_batches = len(batches)

        logger.info(
            f"ğŸš€ é–‹å§‹å„ªåŒ–æ‰¹é‡è™•ç†: {len(filenames)} å€‹æª”æ¡ˆï¼Œ{len(batches)} å€‹æ‰¹æ¬¡"
        )

        successful_receipts = []
        failed_files = []

        for batch_idx, batch_filenames in enumerate(batches):
            self.current_batch = batch_idx + 1
            logger.info(
                f"ğŸ”„ è™•ç†æ‰¹æ¬¡ {self.current_batch}/{self.total_batches}ï¼ŒåŒ…å« {len(batch_filenames)} å€‹æª”æ¡ˆ"
            )

            # ä¸¦è¡Œè™•ç†ç•¶å‰æ‰¹æ¬¡
            batch_results = await self._process_batch_parallel(batch_filenames)

            # è™•ç†çµæœ
            for result in batch_results:
                self.current_progress += 1

                if result.get("success") and result.get("data"):
                    successful_receipts.append(result["data"])
                    logger.info(f"âœ… {result['filename']} è™•ç†æˆåŠŸ")
                else:
                    failed_files.append(
                        {
                            "filename": result.get("filename", "unknown"),
                            "error": result.get("error", "æœªçŸ¥éŒ¯èª¤"),
                        }
                    )
                    logger.error(
                        f"âŒ {result.get('filename', 'unknown')} è™•ç†å¤±æ•—: {result.get('error')}"
                    )

            # æ‰¹æ¬¡é–“å»¶é²ï¼ˆå‹•æ…‹èª¿æ•´ï¼‰
            if batch_idx < len(batches) - 1:
                delay = self._calculate_adaptive_delay(len(batch_filenames))
                logger.info(f"â³ æ‰¹æ¬¡é–“å»¶é²: {delay}ç§’")
                await asyncio.sleep(delay)

        # ä¿å­˜çµæœ
        csv_files = {}
        if successful_receipts:
            csv_files = csv_service.save_consolidated_csv(successful_receipts)
            logger.info(f"ğŸ“Š ä¿å­˜äº† {len(successful_receipts)} å€‹æ”¶æ“šåˆ°CSV")

        # æ¸…ç†å¤±æ•—çš„åœ–ç‰‡ï¼ˆå¦‚æœè¨­å®šç‚ºä¸ä¿ç•™ï¼‰
        await self._cleanup_failed_images(failed_files)

        total_time = time.time() - start_time

        return {
            "success": True,
            "processed_count": len(successful_receipts),
            "failed_count": len(failed_files),
            "failed_files": failed_files,
            "total_time": round(total_time, 2),
            "csv_files": csv_files,
            "avg_time_per_item": (
                round(total_time / len(filenames), 2) if filenames else 0
            ),
            "deleted_successful": (
                len(successful_receipts) if self.auto_delete_successful else 0
            ),
            "deleted_failed": len(failed_files) if not self.keep_failed_files else 0,
        }

    def _calculate_adaptive_delay(self, batch_size: int) -> float:
        """è¨ˆç®—è‡ªé©æ‡‰å»¶é² - ç¢ºä¿ç¬¦åˆAzure F0å…è²»å±¤é™åˆ¶"""
        # Azure F0å…è²»å±¤ï¼šæ¯åˆ†é˜20æ¬¡è«‹æ±‚ï¼Œå³æ¯3ç§’1æ¬¡è«‹æ±‚
        # ç‚ºäº†å®‰å…¨èµ·è¦‹ï¼Œè¨­å®šç‚ºæ¯4ç§’1æ¬¡è«‹æ±‚ï¼ˆæ¯åˆ†é˜15æ¬¡ï¼‰
        min_delay_per_request = 4.0

        # æ ¹æ“šæ‰¹æ¬¡å¤§å°è¨ˆç®—æ‰€éœ€å»¶é²
        required_delay = batch_size * min_delay_per_request

        # æ·»åŠ é¡å¤–çš„å®‰å…¨é‚Šéš›
        safety_margin = 2.0
        total_delay = required_delay + safety_margin

        # ç¢ºä¿å»¶é²åœ¨åˆç†ç¯„åœå…§
        min_delay = 5.0  # æœ€å°‘5ç§’
        max_delay = 30.0  # æœ€å¤š30ç§’

        return max(min_delay, min(total_delay, max_delay))

    def get_progress(self) -> Dict:
        """ç²å–ç•¶å‰é€²åº¦"""
        if self.total_items == 0:
            return {
                "current_progress": 0,
                "total_items": 0,
                "percentage": 0,
                "current_batch": 0,
                "total_batches": 0,
                "estimated_completion": "è¨ˆç®—ä¸­...",
                "elapsed_time": 0,
                "optimization_status": "å·²å•Ÿç”¨",
            }

        percentage = (self.current_progress / self.total_items) * 100
        elapsed_time = time.time() - self.start_time if self.start_time else 0

        # ä¼°ç®—å‰©é¤˜æ™‚é–“
        if self.current_progress > 0:
            avg_time_per_item = elapsed_time / self.current_progress
            remaining_items = self.total_items - self.current_progress
            estimated_remaining = remaining_items * avg_time_per_item

            if estimated_remaining < 60:
                estimated_completion = f"{int(estimated_remaining)}ç§’"
            elif estimated_remaining < 3600:
                estimated_completion = f"{int(estimated_remaining / 60)}åˆ†é˜"
            else:
                estimated_completion = f"{int(estimated_remaining / 3600)}å°æ™‚{int((estimated_remaining % 3600) / 60)}åˆ†é˜"
        else:
            estimated_completion = "è¨ˆç®—ä¸­..."

        return {
            "current_progress": self.current_progress,
            "total_items": self.total_items,
            "percentage": round(percentage, 1),
            "current_batch": self.current_batch,
            "total_batches": self.total_batches,
            "estimated_completion": estimated_completion,
            "elapsed_time": round(elapsed_time, 1),
            "optimization_status": "å·²å•Ÿç”¨",
            "parallel_azure": self.max_concurrent_azure,
            "parallel_claude": self.max_concurrent_claude,
        }


# å‰µå»ºå…¨å±€å¯¦ä¾‹
optimized_batch_processor = OptimizedBatchProcessor()
