"""
æ‰¹æ¬¡è™•ç†æœå‹™ - è™•ç†å¤§é‡åœ–ç‰‡æ™‚çš„é »ç‡æ§åˆ¶å’Œåˆ†æ‰¹è™•ç†
"""

import asyncio
import os
import time
import uuid
from typing import List, Dict, Optional
from loguru import logger
from app.services.ocr_service import ocr_service
from app.services.ai_service import ai_service
from app.services.csv_service import csv_service
from app.services.cache_service import cache_service
from app.services.azure_usage_tracker import azure_usage_tracker
from app.utils.image_utils import image_utils


class BatchProcessor:
    """æ‰¹æ¬¡è™•ç†å™¨ - è™•ç†å¤§é‡åœ–ç‰‡æ™‚çš„é »ç‡æ§åˆ¶"""

    def __init__(self):
        self.rate_limit = 20  # æ¯åˆ†é˜æœ€å¤š20æ¬¡è«‹æ±‚
        self.batch_size = 20  # æ¯æ‰¹æœ€å¤š20å€‹åœ–ç‰‡
        self.delay_between_batches = 60  # æ‰¹æ¬¡é–“éš”60ç§’
        self.delay_between_requests = 3  # è«‹æ±‚é–“éš”3ç§’

        # é€²åº¦è¿½è¹¤
        self.current_progress = 0
        self.total_items = 0
        self.current_batch = 0
        self.total_batches = 0
        self.start_time = None
        self.estimated_completion_time = None

        # æª”æ¡ˆç®¡ç†
        self.auto_delete_successful = True  # è™•ç†æˆåŠŸå¾Œè‡ªå‹•åˆªé™¤åœ–ç‰‡
        self.keep_failed_files = True  # ä¿ç•™å¤±æ•—çš„æª”æ¡ˆä»¥ä¾¿é‡è©¦

    def _calculate_batch_delay(self, batch_size: int) -> float:
        """è¨ˆç®—æ‰¹æ¬¡é–“éœ€è¦çš„å»¶é²æ™‚é–“"""
        # ç¢ºä¿æ¯åˆ†é˜ä¸è¶…é20æ¬¡è«‹æ±‚
        requests_per_minute = batch_size
        if requests_per_minute > self.rate_limit:
            # éœ€è¦å»¶é²ä»¥ç¬¦åˆé™åˆ¶
            delay_needed = (requests_per_minute / self.rate_limit - 1) * 60
            return max(delay_needed, self.delay_between_batches)
        else:
            return self.delay_between_requests * batch_size

    def _estimate_completion_time(self, total_items: int) -> str:
        """ä¼°ç®—å®Œæˆæ™‚é–“"""
        if self.start_time is None:
            return "è¨ˆç®—ä¸­..."

        elapsed_time = time.time() - self.start_time
        if self.current_progress == 0:
            return "è¨ˆç®—ä¸­..."

        # è¨ˆç®—æ¯é …å¹³å‡è™•ç†æ™‚é–“
        avg_time_per_item = elapsed_time / self.current_progress
        remaining_items = total_items - self.current_progress
        estimated_remaining_time = remaining_items * avg_time_per_item

        # åŠ ä¸Šæ‰¹æ¬¡å»¶é²æ™‚é–“
        remaining_batches = self.total_batches - self.current_batch
        batch_delay_time = remaining_batches * self.delay_between_batches

        total_remaining_time = estimated_remaining_time + batch_delay_time

        if total_remaining_time < 60:
            return f"{int(total_remaining_time)}ç§’"
        elif total_remaining_time < 3600:
            return f"{int(total_remaining_time / 60)}åˆ†é˜"
        else:
            return f"{int(total_remaining_time / 3600)}å°æ™‚{int((total_remaining_time % 3600) / 60)}åˆ†é˜"

    def get_progress(self) -> Dict:
        """ç²å–ç•¶å‰é€²åº¦"""
        if self.total_items == 0:
            return {
                "current_progress": 0,
                "total_items": 0,
                "percentage": 0,
                "current_batch": 0,
                "total_batches": 0,
                "estimated_completion": "è¨ˆç®—ä¸­...",
                "elapsed_time": 0,
            }

        percentage = (self.current_progress / self.total_items) * 100
        elapsed_time = time.time() - self.start_time if self.start_time else 0

        return {
            "current_progress": self.current_progress,
            "total_items": self.total_items,
            "percentage": round(percentage, 1),
            "current_batch": self.current_batch,
            "total_batches": self.total_batches,
            "estimated_completion": self._estimate_completion_time(self.total_items),
            "elapsed_time": round(elapsed_time, 1),
        }

    async def process_single_item(
        self, filename: str, enhance_image: bool = True, save_detailed_csv: bool = False
    ) -> Dict:
        """è™•ç†å–®å€‹åœ–ç‰‡"""
        try:
            # æ§‹å»ºæª”æ¡ˆè·¯å¾‘
            file_path = f"./data/receipts/{filename}"

            if not image_utils.validate_image(file_path):
                return {
                    "filename": filename,
                    "success": False,
                    "error": "ç„¡æ•ˆçš„åœ–ç‰‡æª”æ¡ˆ",
                }

            # åœ–ç‰‡é è™•ç†
            processed_image_path = file_path
            if enhance_image:
                processed_image_path = image_utils.enhance_image_quality(file_path)

            # OCRæ–‡å­—è­˜åˆ¥
            logger.info(f"æ‰¹æ¬¡è™•ç† - OCR: {filename}")
            ocr_result = await ocr_service.extract_text(processed_image_path)

            # æå–çµæ§‹åŒ–è³‡æ–™
            structured_data = ocr_service.extract_structured_data(ocr_result)

            # AIæ•´ç†å’Œçµæ§‹åŒ–ï¼ˆæª¢æŸ¥æ˜¯å¦æœ‰æš«å­˜ï¼‰
            logger.info(f"æ‰¹æ¬¡è™•ç† - AI: {filename}")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰AIæš«å­˜
            ai_cache_data = cache_service.load_ai_result(filename)
            if ai_cache_data and ai_cache_data.get("receipt_data"):
                logger.info(f"ä½¿ç”¨AIæš«å­˜è³‡æ–™: {filename}")
                # å¾æš«å­˜è³‡æ–™æ¢å¾©ReceiptDataå°è±¡
                from app.models.receipt import ReceiptData
                receipt_dict = ai_cache_data["receipt_data"]
                # è™•ç†æ—¥æœŸå­—ä¸²
                if isinstance(receipt_dict.get("date"), str):
                    from datetime import datetime
                    try:
                        receipt_dict["date"] = datetime.fromisoformat(receipt_dict["date"])
                    except:
                        pass
                receipt_data = ReceiptData(**receipt_dict)
            else:
                # åŸ·è¡ŒAIè™•ç†
                receipt_data = await ai_service.process_receipt_text(
                    ocr_result, structured_data
                )
                # ä¿å­˜åˆ°æš«å­˜
                cache_service.save_ai_result(filename, receipt_data, ocr_result)

            # è¨­å®šä¾†æºåœ–ç‰‡
            receipt_data.source_image = filename

            # ä¸ç«‹å³å„²å­˜CSVï¼Œè€Œæ˜¯æ”¶é›†çµæœ
            # csv_service.save_receipt_to_csv(receipt_data)
            #
            # # å¦‚æœéœ€è¦ï¼Œå„²å­˜è©³ç´°CSV
            # if save_detailed_csv:
            #     csv_service.save_detailed_csv(receipt_data)

            # è™•ç†æˆåŠŸå¾Œåˆªé™¤åœ–ç‰‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.auto_delete_successful:
                await self._delete_successful_image(filename)

            return {"filename": filename, "success": True, "data": receipt_data}

        except Exception as e:
            logger.error(f"æ‰¹æ¬¡è™•ç†å¤±æ•—: {filename}, éŒ¯èª¤: {str(e)}")
            return {"filename": filename, "success": False, "error": str(e)}

    async def _delete_successful_image(self, filename: str):
        """åˆªé™¤è™•ç†æˆåŠŸçš„åœ–ç‰‡"""
        try:
            image_path = f"./data/receipts/{filename}"
            if os.path.exists(image_path):
                os.remove(image_path)
                logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤è™•ç†æˆåŠŸçš„åœ–ç‰‡: {filename}")
            else:
                logger.warning(f"åœ–ç‰‡ä¸å­˜åœ¨ï¼Œç„¡æ³•åˆªé™¤: {filename}")
        except Exception as e:
            logger.error(f"åˆªé™¤åœ–ç‰‡å¤±æ•— {filename}: {e}")

    async def _cleanup_failed_images(self, failed_files: List[Dict]):
        """æ¸…ç†å¤±æ•—çš„åœ–ç‰‡ï¼ˆå¦‚æœè¨­å®šç‚ºä¸ä¿ç•™ï¼‰"""
        if self.keep_failed_files:
            return

        for failed_file in failed_files:
            filename = failed_file.get("filename")
            if filename:
                try:
                    image_path = f"./data/receipts/{filename}"
                    if os.path.exists(image_path):
                        os.remove(image_path)
                        logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤å¤±æ•—çš„åœ–ç‰‡: {filename}")
                except Exception as e:
                    logger.error(f"åˆªé™¤å¤±æ•—åœ–ç‰‡æ™‚å‡ºéŒ¯ {filename}: {e}")

    async def process_batch(
        self,
        filenames: List[str],
        enhance_image: bool = True,
        save_detailed_csv: bool = False,
    ) -> List[Dict]:
        """è™•ç†ä¸€å€‹æ‰¹æ¬¡"""
        batch_results = []

        for i, filename in enumerate(filenames):
            logger.info(f"   è™•ç†æª”æ¡ˆ {i+1}/{len(filenames)}: {filename}")

            # è™•ç†å–®å€‹åœ–ç‰‡
            result = await self.process_single_item(
                filename, enhance_image, save_detailed_csv
            )
            batch_results.append(result)

            # æ›´æ–°é€²åº¦
            self.current_progress += 1

            logger.info(
                f"   æª”æ¡ˆ {filename} è™•ç†å®Œæˆ: {'æˆåŠŸ' if result['success'] else 'å¤±æ•—'}"
            )

            # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å€‹ï¼Œæ·»åŠ è«‹æ±‚é–“éš”
            if i < len(filenames) - 1:
                logger.info(f"   ç­‰å¾… {self.delay_between_requests} ç§’...")
                await asyncio.sleep(self.delay_between_requests)

        return batch_results

    async def process_large_batch(
        self,
        filenames: List[str],
        enhance_image: bool = True,
        save_detailed_csv: bool = False,
    ) -> Dict:
        """è™•ç†å¤§é‡åœ–ç‰‡ï¼ŒåŒ…å«é »ç‡æ§åˆ¶"""
        self.start_time = time.time()
        self.total_items = len(filenames)
        self.current_progress = 0
        self.current_batch = 0

        # åˆ†æ‰¹è™•ç†
        batches = [
            filenames[i : i + self.batch_size]
            for i in range(0, len(filenames), self.batch_size)
        ]
        self.total_batches = len(batches)

        all_results = []
        failed_files = []
        successful_receipts = []  # æ”¶é›†æˆåŠŸçš„æ”¶æ“šè³‡æ–™

        logger.info(f"é–‹å§‹æ‰¹æ¬¡è™•ç† {len(filenames)} å€‹æª”æ¡ˆï¼Œåˆ†ç‚º {len(batches)} å€‹æ‰¹æ¬¡")
        logger.info(f"ğŸ“Š æ‰¹æ¬¡åˆ†é…:")
        for i, batch in enumerate(batches):
            logger.info(f"   æ‰¹æ¬¡ {i+1}: {len(batch)} å€‹æª”æ¡ˆ - {batch}")

        for batch_index, batch_filenames in enumerate(batches):
            self.current_batch = batch_index + 1

            logger.info(
                f"ğŸ”„ è™•ç†æ‰¹æ¬¡ {self.current_batch}/{self.total_batches}ï¼ŒåŒ…å« {len(batch_filenames)} å€‹æª”æ¡ˆ"
            )
            logger.info(f"   æ‰¹æ¬¡æª”æ¡ˆ: {batch_filenames}")

            # è™•ç†ç•¶å‰æ‰¹æ¬¡
            batch_results = await self.process_batch(
                batch_filenames, enhance_image, save_detailed_csv
            )
            all_results.extend(batch_results)

            # æ”¶é›†å¤±æ•—çš„æª”æ¡ˆå’ŒæˆåŠŸçš„æ”¶æ“šè³‡æ–™
            for result in batch_results:
                if result["success"]:
                    successful_receipts.append(result["data"])
                else:
                    failed_files.append(
                        {"filename": result["filename"], "error": result["error"]}
                    )

            # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å€‹æ‰¹æ¬¡ï¼Œæ·»åŠ å»¶é²
            if batch_index < len(batches) - 1:
                delay_time = self._calculate_batch_delay(len(batch_filenames))
                logger.info(
                    f"æ‰¹æ¬¡ {self.current_batch} å®Œæˆï¼Œç­‰å¾… {delay_time:.1f} ç§’å¾Œè™•ç†ä¸‹ä¸€æ‰¹æ¬¡..."
                )
                await asyncio.sleep(delay_time)

        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        total_time = time.time() - self.start_time

        # çµ±è¨ˆçµæœ
        processed_count = len([r for r in all_results if r["success"]])
        failed_count = len(failed_files)

        # å‰µå»ºæ•´åˆCSVæª”æ¡ˆ
        csv_files = {}
        if successful_receipts:
            try:
                csv_files = csv_service.save_consolidated_csv(successful_receipts)
                logger.info(f"æ•´åˆCSVæª”æ¡ˆå·²å‰µå»º: {csv_files}")
            except Exception as e:
                logger.error(f"å‰µå»ºæ•´åˆCSVå¤±æ•—: {str(e)}")

        logger.info(f"æ‰¹æ¬¡è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {total_time:.2f}ç§’")
        logger.info(f"æˆåŠŸ: {processed_count}, å¤±æ•—: {failed_count}")

        # æ¸…ç†å¤±æ•—çš„åœ–ç‰‡ï¼ˆå¦‚æœè¨­å®šç‚ºä¸ä¿ç•™ï¼‰
        await self._cleanup_failed_images(failed_files)

        return {
            "success": True,
            "processed_count": processed_count,
            "failed_count": failed_count,
            "results": all_results,
            "failed_files": failed_files,
            "csv_files": csv_files,  # æ·»åŠ CSVæª”æ¡ˆè·¯å¾‘
            "total_time": round(total_time, 2),
            "message": f"æ‰¹æ¬¡è™•ç†å®Œæˆã€‚æˆåŠŸ: {processed_count}, å¤±æ•—: {failed_count}, è€—æ™‚: {total_time:.2f}ç§’",
            "deleted_successful": processed_count if self.auto_delete_successful else 0,
            "deleted_failed": failed_count if not self.keep_failed_files else 0,
        }

    async def process_ocr_only(
        self, filenames: List[str], enhance_image: bool = True
    ) -> Dict:
        """
        åªåŸ·è¡ŒOCRè™•ç†ï¼Œçµæœæš«å­˜

        Args:
            filenames: æª”æ¡ˆåç¨±åˆ—è¡¨
            enhance_image: æ˜¯å¦å¢å¼·åœ–ç‰‡å“è³ª

        Returns:
            è™•ç†çµæœ
        """
        batch_id = str(uuid.uuid4())
        self.start_time = time.time()
        self.total_items = len(filenames)
        self.current_progress = 0

        logger.info(f"é–‹å§‹OCRè™•ç† {len(filenames)} å€‹æª”æ¡ˆï¼Œæ‰¹æ¬¡ID: {batch_id}")

        ocr_results = []
        failed_files = []

        for i, filename in enumerate(filenames):
            try:
                # æ›´æ–°é€²åº¦
                self.current_progress = i + 1

                # é©—è­‰åœ–ç‰‡
                from app.config import settings

                file_path = os.path.join(settings.upload_dir, filename)
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"æª”æ¡ˆä¸å­˜åœ¨: {filename}")

                image_utils.validate_image(file_path)

                # å¢å¼·åœ–ç‰‡å“è³ª
                if enhance_image:
                    enhanced_path = image_utils.enhance_image_quality(file_path)
                    process_path = enhanced_path
                else:
                    process_path = file_path

                # åŸ·è¡ŒOCR
                logger.info(f"OCRè™•ç†: {filename}")
                ocr_data = ocr_service.extract_text(process_path)

                # æš«å­˜OCRçµæœ
                cache_path = cache_service.save_ocr_result(filename, ocr_data)

                ocr_results.append(
                    {
                        "filename": filename,
                        "success": True,
                        "ocr_data": ocr_data,
                        "cache_path": cache_path,
                    }
                )

                logger.info(f"OCRå®Œæˆä¸¦æš«å­˜: {filename}")

            except Exception as e:
                logger.error(f"OCRè™•ç†å¤±æ•—: {filename}, éŒ¯èª¤: {str(e)}")
                failed_files.append({"filename": filename, "error": str(e)})
                ocr_results.append(
                    {"filename": filename, "success": False, "error": str(e)}
                )

            # æ·»åŠ è«‹æ±‚é–“éš”
            if i < len(filenames) - 1:
                await asyncio.sleep(self.delay_between_requests)

        # å„²å­˜è™•ç†ç‹€æ…‹
        status = {
            "batch_id": batch_id,
            "total_files": len(filenames),
            "ocr_success": len([r for r in ocr_results if r["success"]]),
            "ocr_failed": len(failed_files),
            "cache_files": [r["cache_path"] for r in ocr_results if r["success"]],
            "timestamp": time.time(),
        }
        cache_service.save_processing_status(batch_id, status)

        total_time = time.time() - self.start_time

        return {
            "success": True,
            "batch_id": batch_id,
            "processed_count": len([r for r in ocr_results if r["success"]]),
            "failed_count": len(failed_files),
            "results": ocr_results,
            "failed_files": failed_files,
            "total_time": round(total_time, 2),
            "message": f"OCRè™•ç†å®Œæˆã€‚æˆåŠŸ: {len([r for r in ocr_results if r['success']])}, å¤±æ•—: {len(failed_files)}, è€—æ™‚: {total_time:.2f}ç§’",
        }

    async def process_from_cache(
        self, batch_id: str, save_detailed_csv: bool = False
    ) -> Dict:
        """
        å¾æš«å­˜è™•ç†AIåˆ†æ

        Args:
            batch_id: æ‰¹é‡è™•ç†ID
            save_detailed_csv: æ˜¯å¦å„²å­˜è©³ç´°CSV

        Returns:
            è™•ç†çµæœ
        """
        # è¼‰å…¥è™•ç†ç‹€æ…‹
        status_data = cache_service.load_processing_status(batch_id)
        if not status_data:
            return {"success": False, "error": f"æ‰¾ä¸åˆ°æ‰¹æ¬¡ID: {batch_id}"}

        cache_files = status_data["status"]["cache_files"]
        logger.info(
            f"å¾æš«å­˜è™•ç†AIåˆ†æï¼Œæ‰¹æ¬¡ID: {batch_id}, æš«å­˜æª”æ¡ˆæ•¸: {len(cache_files)}"
        )

        self.start_time = time.time()
        self.total_items = len(cache_files)
        self.current_progress = 0

        ai_results = []
        successful_receipts = []
        failed_files = []

        for i, cache_path in enumerate(cache_files):
            try:
                # æ›´æ–°é€²åº¦
                self.current_progress = i + 1

                # è¼‰å…¥OCRçµæœ
                cache_data = cache_service.load_ocr_result(cache_path)
                if not cache_data:
                    raise Exception(f"ç„¡æ³•è¼‰å…¥æš«å­˜è³‡æ–™: {cache_path}")

                filename = cache_data["filename"]
                ocr_data = cache_data["ocr_data"]

                # AIè™•ç†
                logger.info(f"AIè™•ç†: {filename}")
                receipt_data = ai_service.process_receipt_text(ocr_data["text"])
                receipt_data.source_image = filename

                successful_receipts.append(receipt_data)
                ai_results.append(
                    {"filename": filename, "success": True, "data": receipt_data}
                )

                logger.info(f"AIè™•ç†å®Œæˆ: {filename}")

            except Exception as e:
                logger.error(f"AIè™•ç†å¤±æ•—: {cache_path}, éŒ¯èª¤: {str(e)}")
                failed_files.append({"filename": cache_path, "error": str(e)})
                ai_results.append(
                    {"filename": cache_path, "success": False, "error": str(e)}
                )

            # æ·»åŠ è«‹æ±‚é–“éš”
            if i < len(cache_files) - 1:
                await asyncio.sleep(self.delay_between_requests)

        # å‰µå»ºæ•´åˆCSVæª”æ¡ˆ
        csv_files = {}
        if successful_receipts:
            try:
                csv_files = csv_service.save_consolidated_csv(successful_receipts)
                logger.info(f"æ•´åˆCSVæª”æ¡ˆå·²å‰µå»º: {csv_files}")
            except Exception as e:
                logger.error(f"å‰µå»ºæ•´åˆCSVå¤±æ•—: {str(e)}")

        total_time = time.time() - self.start_time

        return {
            "success": True,
            "batch_id": batch_id,
            "processed_count": len([r for r in ai_results if r["success"]]),
            "failed_count": len(failed_files),
            "results": ai_results,
            "failed_files": failed_files,
            "csv_files": csv_files,
            "total_time": round(total_time, 2),
            "message": f"AIè™•ç†å®Œæˆã€‚æˆåŠŸ: {len([r for r in ai_results if r['success']])}, å¤±æ•—: {len(failed_files)}, è€—æ™‚: {total_time:.2f}ç§’",
        }

    async def merge_with_existing_csv(
        self, new_receipts: List, existing_csv_path: str = None
    ) -> Dict:
        """
        å°‡æ–°çš„æ”¶æ“šè³‡æ–™åˆä½µåˆ°ç¾æœ‰CSVæª”æ¡ˆ

        Args:
            new_receipts: æ–°çš„æ”¶æ“šè³‡æ–™åˆ—è¡¨
            existing_csv_path: ç¾æœ‰CSVæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰

        Returns:
            åˆä½µçµæœ
        """
        try:
            # è¼‰å…¥ç¾æœ‰æ”¶æ“šè³‡æ–™
            existing_receipts = []
            if existing_csv_path and os.path.exists(existing_csv_path):
                existing_receipts = csv_service.load_receipts_from_csv(
                    existing_csv_path
                )
                logger.info(f"è¼‰å…¥ç¾æœ‰æ”¶æ“š: {len(existing_receipts)} ç­†")

            # åˆä½µæ”¶æ“šè³‡æ–™
            all_receipts = existing_receipts + new_receipts
            logger.info(f"åˆä½µå¾Œç¸½æ”¶æ“šæ•¸: {len(all_receipts)} ç­†")

            # å‰µå»ºæ–°çš„æ•´åˆCSV
            csv_files = csv_service.save_consolidated_csv(all_receipts)

            return {
                "success": True,
                "existing_count": len(existing_receipts),
                "new_count": len(new_receipts),
                "total_count": len(all_receipts),
                "csv_files": csv_files,
                "message": f"åˆä½µå®Œæˆã€‚åŸæœ‰: {len(existing_receipts)}, æ–°å¢: {len(new_receipts)}, ç¸½è¨ˆ: {len(all_receipts)}",
            }

        except Exception as e:
            logger.error(f"åˆä½µCSVå¤±æ•—: {str(e)}")
            return {"success": False, "error": str(e)}


# å…¨å±€å¯¦ä¾‹
batch_processor = BatchProcessor()
